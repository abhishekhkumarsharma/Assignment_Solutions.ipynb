{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekhkumarsharma/Assignment_Solutions.ipynb/blob/main/DA_AG_016_KNN_PCA_Assignment_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-016"
      ],
      "metadata": {
        "id": "azc_aeQ842Ed"
      },
      "id": "azc_aeQ842Ed"
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN & PCA | Assignment"
      ],
      "metadata": {
        "id": "RC1jrsxp5OXq"
      },
      "id": "RC1jrsxp5OXq"
    },
    {
      "cell_type": "markdown",
      "id": "901211b7",
      "metadata": {
        "id": "901211b7"
      },
      "source": [
        "**Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?**\n",
        "\n",
        "**Answer:**  \n",
        "K-Nearest Neighbors (KNN) is a *lazy*, instance-based learning algorithm that makes predictions based on the similarity between a new data point and points in the training set. It does **not** explicitly learn a parametric model; instead, it stores the training data and uses a distance metric to make predictions at query time.\n",
        "\n",
        "KNN relies on three key ideas:\n",
        "1. A **distance metric** (e.g., Euclidean, Manhattan) to measure similarity between data points.  \n",
        "2. A hyperparameter **K**, the number of nearest neighbors to consider.  \n",
        "3. A **voting or averaging rule** to make the final prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### KNN for Classification\n",
        "In classification, the goal is to assign a **class label** to a new instance.\n",
        "\n",
        "**Steps:**\n",
        "1. Choose a value of **K** (e.g., K = 3, 5, 7).  \n",
        "2. Compute the distance between the new point and **all** points in the training set.  \n",
        "3. Select the **K closest** training points (nearest neighbors).  \n",
        "4. Perform **majority voting**: the class that appears most frequently among the K neighbors is assigned as the predicted class.  \n",
        "5. Optionally, use **distance-weighted voting**, where closer neighbors get higher weights.\n",
        "\n",
        "**Example:**  \n",
        "If K = 5 and among the 5 nearest neighbors we have 3 samples of class A and 2 of class B, the new point is classified as **class A**.\n",
        "\n",
        "---\n",
        "\n",
        "### KNN for Regression\n",
        "In regression, the goal is to predict a **continuous value** (e.g., house price).\n",
        "\n",
        "**Steps:**\n",
        "1. Choose K and a distance metric.  \n",
        "2. Find the K nearest neighbors of the query point.  \n",
        "3. Instead of majority voting, compute the **average (or weighted average)** of the target values of these neighbors.  \n",
        "4. This average is returned as the predicted continuous output.\n",
        "\n",
        "**Example:**  \n",
        "If K = 3 and the target values of neighbors are 10, 12, 14, the prediction is (10 + 12 + 14) / 3 = 12.\n",
        "\n",
        "---\n",
        "\n",
        "### Characteristics of KNN\n",
        "- **Advantages:**\n",
        "  - Simple to understand and implement.\n",
        "  - Naturally handles multi-class problems.\n",
        "  - Non-parametric: can learn very complex decision boundaries if enough data is available.\n",
        "\n",
        "- **Disadvantages:**\n",
        "  - **Computationally expensive** at prediction time (needs distances to all training points).\n",
        "  - Sensitive to the **choice of K** and **feature scaling**.\n",
        "  - Performance degrades in **high-dimensional spaces** due to the Curse of Dimensionality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45ad4f67",
      "metadata": {
        "id": "45ad4f67"
      },
      "source": [
        "**Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?**\n",
        "\n",
        "**Answer:**  \n",
        "The **Curse of Dimensionality** refers to a set of problems that arise when working with data in very high-dimensional spaces (with many features). As the number of dimensions increases, the volume of the space grows so fast that the available data becomes **sparse**, and many intuitive notions like distance, density, and nearest neighbors become less meaningful.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Aspects of the Curse of Dimensionality\n",
        "\n",
        "1. **Distance Concentration:**  \n",
        "   In high dimensions, the distances between points tend to become very similar. The ratio between the distance to the nearest neighbor and the farthest neighbor approaches 1. As a result, it becomes difficult to distinguish which points are truly “close” or “far” from a query point.\n",
        "\n",
        "2. **Data Sparsity:**  \n",
        "   To represent the space adequately in high dimensions, we need an **exponentially larger** number of samples. With a limited dataset, points are spread thinly across the space, making local neighborhoods less reliable.\n",
        "\n",
        "3. **Overfitting Risk Increases:**  \n",
        "   Models that depend heavily on local neighborhoods (like KNN) may overfit because the neighbors used for prediction may not be truly similar in a meaningful sense.\n",
        "\n",
        "---\n",
        "\n",
        "### How the Curse of Dimensionality Affects KNN\n",
        "\n",
        "1. **Less Meaningful Neighbors:**  \n",
        "   KNN relies on the idea that *nearby points are similar*. In high-dimensional spaces, due to distance concentration, the notion of “nearest neighbors” loses its discriminative power. The nearest neighbors may not be meaningfully similar to the query point, leading to poor predictions.\n",
        "\n",
        "2. **Higher Variance and Overfitting:**  \n",
        "   Because local neighborhoods are unreliable, the model’s predictions become noisy and unstable. Small changes in data can drastically change which neighbors are selected.\n",
        "\n",
        "3. **Increased Computational Cost:**  \n",
        "   - More features → higher computational cost for distance calculations.  \n",
        "   - If many features are irrelevant or noisy, they add distance noise and reduce model performance.\n",
        "\n",
        "4. **Need for Dimensionality Reduction / Feature Selection:**  \n",
        "   To combat the Curse of Dimensionality, we often use methods like **PCA** (Principal Component Analysis) or **feature selection** to reduce the number of dimensions before applying KNN.\n",
        "\n",
        "---\n",
        "\n",
        "**Summary:**  \n",
        "The Curse of Dimensionality makes distances less informative and neighborhoods less meaningful in high-dimensional spaces. Since KNN depends directly on distances between data points, its performance can degrade severely when the number of features is very large unless we perform dimensionality reduction or careful feature engineering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8abf96cb",
      "metadata": {
        "id": "8abf96cb"
      },
      "source": [
        "**Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?**\n",
        "\n",
        "**Answer:**  \n",
        "**Principal Component Analysis (PCA)** is an **unsupervised linear dimensionality reduction technique**. It transforms the original features into a new set of uncorrelated features called **principal components**. These components are linear combinations of the original features and are ordered in such a way that:\n",
        "- The **first principal component** captures the maximum possible variance in the data.\n",
        "- The **second principal component** captures the maximum remaining variance, subject to being orthogonal (uncorrelated) to the first, and so on.\n",
        "\n",
        "PCA is used to:\n",
        "- Reduce dimensionality while preserving most of the variance (information) in the data.\n",
        "- Remove multicollinearity (correlation) between features.\n",
        "- Visualize high-dimensional data in 2D or 3D.\n",
        "\n",
        "---\n",
        "\n",
        "### How PCA Works (High-Level)\n",
        "1. Standardize the data (mean 0, variance 1) if features are on different scales.  \n",
        "2. Compute the **covariance matrix** of the features.  \n",
        "3. Compute the **eigenvalues and eigenvectors** of the covariance matrix.  \n",
        "4. Sort eigenvectors in decreasing order of their eigenvalues (variance explained).  \n",
        "5. Select the top *k* eigenvectors to form a projection matrix.  \n",
        "6. Project the original data onto this lower-dimensional space to obtain the principal components.\n",
        "\n",
        "---\n",
        "\n",
        "### PCA vs Feature Selection\n",
        "\n",
        "Although PCA and feature selection both aim to reduce dimensionality, they are fundamentally different:\n",
        "\n",
        "#### 1. **PCA = Feature Extraction, Not Feature Selection**\n",
        "- PCA creates **new features** (principal components) that are combinations of the original features.\n",
        "- These new features are **not directly interpretable** in terms of the original variables.\n",
        "\n",
        "#### 2. **Feature Selection Keeps Original Features**\n",
        "- Feature selection methods choose a **subset of existing features** based on certain criteria:\n",
        "  - Filter methods (e.g., correlation, mutual information).\n",
        "  - Wrapper methods (e.g., recursive feature elimination).\n",
        "  - Embedded methods (e.g., LASSO).\n",
        "- The selected features remain **unchanged and interpretable**.\n",
        "\n",
        "#### 3. **Supervised vs Unsupervised**\n",
        "- PCA is typically **unsupervised**: it does **not** use the target labels when creating components; it only looks at variance in the predictors.  \n",
        "- Many feature selection methods are **supervised**, using the relationship between features and the target variable (e.g., selecting features highly correlated with the label).\n",
        "\n",
        "#### 4. **Goal Difference**\n",
        "- PCA’s goal is to **capture maximum variance** in a smaller number of transformed features.  \n",
        "- Feature selection’s goal is to **retain the most relevant original features** for prediction, interpretability, or generalization.\n",
        "\n",
        "---\n",
        "\n",
        "**Summary:**  \n",
        "PCA is a **feature extraction** method that builds new, uncorrelated features (principal components) from linear combinations of existing ones, while feature selection simply picks a subset of the original features without altering them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b3fcb15",
      "metadata": {
        "id": "9b3fcb15"
      },
      "source": [
        "**Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?**\n",
        "\n",
        "**Answer:**  \n",
        "In PCA, **eigenvalues** and **eigenvectors** come from the **covariance matrix** (or correlation matrix) of the data.\n",
        "\n",
        "- The **covariance matrix** summarizes how each pair of features varies together.  \n",
        "- PCA finds directions in feature space along which the data varies the most; these directions are determined by eigenvectors and eigenvalues.\n",
        "\n",
        "---\n",
        "\n",
        "### Eigenvectors in PCA\n",
        "An **eigenvector** of a matrix is a non-zero vector that, when the matrix is applied to it, **only gets scaled** and does not change direction. Formally, for a matrix \\( A \\):\n",
        "\\[\n",
        "A \\mathbf{v} = \\lambda \\mathbf{v}\n",
        "\\]\n",
        "where:\n",
        "- \\( \\mathbf{v} \\) is the eigenvector,\n",
        "- \\( \\lambda \\) is the corresponding eigenvalue.\n",
        "\n",
        "In PCA:\n",
        "- Each eigenvector of the covariance matrix represents a **direction** in the original feature space.  \n",
        "- These directions are the **principal components**.  \n",
        "- The eigenvectors are orthogonal (uncorrelated) to each other.\n",
        "\n",
        "---\n",
        "\n",
        "### Eigenvalues in PCA\n",
        "The **eigenvalue** associated with an eigenvector indicates the **amount of variance** captured along that eigenvector (principal component).\n",
        "\n",
        "- Larger eigenvalue → more variance along that component → more important component.  \n",
        "- Smaller eigenvalue → less variance → less important component.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Eigenvalues and Eigenvectors are Important in PCA\n",
        "\n",
        "1. **Defining Principal Components:**\n",
        "   - The **eigenvectors** of the covariance matrix are the **axes** (directions) of the new feature space (principal components).\n",
        "   - The **eigenvalues** tell us how much of the data’s variance is captured by each component.\n",
        "\n",
        "2. **Ranking Components:**\n",
        "   - PCA sorts components in descending order of their eigenvalues.  \n",
        "   - We keep the top *k* components with the **largest eigenvalues** to get the most informative lower-dimensional representation.\n",
        "\n",
        "3. **Explained Variance Ratio:**\n",
        "   - The **explained variance ratio** for a principal component is:\n",
        "     \\[\n",
        "     \\text{Explained Variance Ratio}_i = \\frac{\\lambda_i}{\\sum_j \\lambda_j}\n",
        "     \\]\n",
        "   - This tells us what fraction of the total variance is captured by each component.\n",
        "\n",
        "4. **Dimensionality Reduction Decision:**\n",
        "   - By examining eigenvalues or the cumulative explained variance ratio, we can decide **how many components to keep** to preserve, for example, 90–95% of the total variance.\n",
        "\n",
        "---\n",
        "\n",
        "**Summary:**  \n",
        "In PCA, eigenvectors give the **directions** of maximum variance (principal components), and eigenvalues tell us **how much variance** is captured along each of those directions. They are central to ranking and selecting the most informative components for dimensionality reduction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bb7e64f",
      "metadata": {
        "id": "4bb7e64f"
      },
      "source": [
        "**Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?**\n",
        "\n",
        "**Answer:**  \n",
        "KNN and PCA complement each other very well, especially in **high-dimensional** datasets, by combining **dimensionality reduction** (PCA) with a **distance-based classifier** (KNN).\n",
        "\n",
        "---\n",
        "\n",
        "### Why PCA Helps KNN\n",
        "\n",
        "1. **Mitigating the Curse of Dimensionality:**\n",
        "   - KNN performance degrades in high-dimensional spaces because distances become less meaningful.\n",
        "   - PCA reduces the number of dimensions while retaining most of the variance (information), making distances more informative.\n",
        "\n",
        "2. **Noise Reduction:**\n",
        "   - PCA tends to discard components with very low variance, which often correspond to **noise** or redundant information.\n",
        "   - KNN then operates on a cleaner, denoised representation of the data, improving generalization.\n",
        "\n",
        "3. **Computational Efficiency:**\n",
        "   - KNN requires computing distances to all training samples at prediction time.\n",
        "   - Reducing the number of dimensions with PCA makes each distance computation cheaper, improving efficiency.\n",
        "\n",
        "4. **Handling Correlated Features:**\n",
        "   - PCA constructs **uncorrelated (orthogonal)** components from correlated features.\n",
        "   - This often leads to more stable distance computations in KNN.\n",
        "\n",
        "---\n",
        "\n",
        "### Typical PCA + KNN Pipeline\n",
        "\n",
        "1. **Standardize Features:**  \n",
        "   Scale features to have zero mean and unit variance (important before PCA and KNN).\n",
        "\n",
        "2. **Apply PCA:**  \n",
        "   - Fit PCA on the training data and choose the number of components (e.g., enough to explain 95% variance).  \n",
        "   - Transform both training and test data into the PCA space.\n",
        "\n",
        "3. **Train KNN in PCA Space:**  \n",
        "   - Train KNN on the PCA-transformed training data.  \n",
        "   - Tune K (and possibly distance metric) using cross-validation.\n",
        "\n",
        "4. **Predict on New Data:**  \n",
        "   - Apply the same scaling and PCA transformation to new data.  \n",
        "   - Use the trained KNN model to make predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### Benefits of a Unified Pipeline (e.g., with scikit-learn)\n",
        "\n",
        "Using `sklearn.pipeline.Pipeline` to chain **StandardScaler → PCA → KNN**:\n",
        "\n",
        "- Ensures consistent preprocessing on train and test data.  \n",
        "- Reduces data leakage by fitting PCA only on training data.  \n",
        "- Allows joint hyperparameter tuning (e.g., number of principal components and K) via grid search or cross-validation.\n",
        "\n",
        "---\n",
        "\n",
        "**Summary:**  \n",
        "PCA reduces dimensionality and noise, making KNN’s distance-based reasoning more effective, especially on high-dimensional datasets. Together, they form a powerful pipeline for classification tasks where interpretability of individual features is less critical than predictive performance and robustness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319857dc",
      "metadata": {
        "id": "319857dc"
      },
      "source": [
        "**Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.  \n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n",
        "**Answer:**  \n",
        "Below is the Python code that:\n",
        "\n",
        "1. Loads the Wine dataset.  \n",
        "2. Splits the data into training and test sets.  \n",
        "3. Trains a KNN classifier **without scaling**.  \n",
        "4. Trains a KNN classifier **with StandardScaler-based feature scaling**.  \n",
        "5. Compares and prints the accuracies in both cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cf955df3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf955df3",
        "outputId": "50623fa8-b490-425a-ed17-5803a98ac288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling : 0.8056\n",
            "Accuracy with scaling    : 0.9722\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 1. KNN without feature scaling\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# 2. KNN with feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = knn_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "print(\"Accuracy without scaling : {:.4f}\".format(accuracy_no_scaling))\n",
        "print(\"Accuracy with scaling    : {:.4f}\".format(accuracy_scaling))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37fd514b",
      "metadata": {
        "id": "37fd514b"
      },
      "source": [
        "**Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.  \n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n",
        "**Answer:**  \n",
        "In the code below, we:\n",
        "\n",
        "1. Load and scale the Wine dataset.  \n",
        "2. Fit PCA with all components.  \n",
        "3. Print the **explained variance ratio** for each principal component.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5149103c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5149103c",
        "outputId": "a5218ab3-c740-4a7f-c710-c0d494cf4693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reuse scaled data for consistency (if running fresh, scale again)\n",
        "scaler_pca = StandardScaler()\n",
        "X_scaled_full = scaler_pca.fit_transform(X)\n",
        "\n",
        "# Fit PCA with all components\n",
        "pca_full = PCA()\n",
        "pca_full.fit(X_scaled_full)\n",
        "\n",
        "explained_variance_ratio = pca_full.explained_variance_ratio_\n",
        "\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "for idx, ratio in enumerate(explained_variance_ratio, start=1):\n",
        "    print(f\"PC{idx}: {ratio:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48d0a6c4",
      "metadata": {
        "id": "48d0a6c4"
      },
      "source": [
        "**Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.**\n",
        "\n",
        "**Answer:**  \n",
        "Below, we:\n",
        "\n",
        "1. Use PCA to reduce the Wine dataset to **2 principal components**.  \n",
        "2. Train KNN on:\n",
        "   - The original **scaled** features.  \n",
        "   - The **2D PCA-transformed** features.  \n",
        "3. Compare the accuracies in both cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e9df7faa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9df7faa",
        "outputId": "3b3ad6e9-4015-45f4-b505-f86064c40423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original scaled features : 0.9722\n",
            "Accuracy on top 2 PCA components     : 0.9167\n"
          ]
        }
      ],
      "source": [
        "# PCA to retain top 2 components\n",
        "pca_2 = PCA(n_components=2)\n",
        "X_train_pca2 = pca_2.fit_transform(X_train_scaled)\n",
        "X_test_pca2 = pca_2.transform(X_test_scaled)\n",
        "\n",
        "# KNN on original scaled data (already trained above as knn_scaling)\n",
        "# Corrected: Use acc_euclidean which is equivalent to accuracy_scaling from previous executions\n",
        "accuracy_original_scaled = acc_euclidean\n",
        "\n",
        "# KNN on PCA (2 components)\n",
        "knn_pca2 = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca2.fit(X_train_pca2, y_train)\n",
        "y_pred_pca2 = knn_pca2.predict(X_test_pca2)\n",
        "accuracy_pca2 = accuracy_score(y_test, y_pred_pca2)\n",
        "\n",
        "print(\"Accuracy on original scaled features : {:.4f}\".format(accuracy_original_scaled))\n",
        "print(\"Accuracy on top 2 PCA components     : {:.4f}\".format(accuracy_pca2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eebfcc67",
      "metadata": {
        "id": "eebfcc67"
      },
      "source": [
        "**Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.  \n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n",
        "**Answer:**  \n",
        "Below, we train KNN on the **scaled Wine dataset** using:\n",
        "\n",
        "- **Euclidean distance** (the default metric in KNN).  \n",
        "- **Manhattan distance** (L1).  \n",
        "\n",
        "We then compare the accuracies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0fbcc631",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fbcc631",
        "outputId": "023ac05e-ef6d-46d7-f4bf-64140bc260e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance : 0.9722\n",
            "Accuracy with Manhattan distance : 1.0000\n"
          ]
        }
      ],
      "source": [
        "# KNN with Euclidean distance (default)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# KNN with Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=1)\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(\"Accuracy with Euclidean distance : {:.4f}\".format(acc_euclidean))\n",
        "print(\"Accuracy with Manhattan distance : {:.4f}\".format(acc_manhattan))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dc9fef8",
        "outputId": "24108302-12eb-4636-ab00-252d75eb7ca8"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature scaling (from cell cf955df3)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# KNN with Euclidean distance (from cell 0fbcc631)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# KNN with Manhattan distance (from cell 0fbcc631)\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=1)\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(\"Accuracy with Euclidean distance : {:.4f}\".format(acc_euclidean))\n",
        "print(\"Accuracy with Manhattan distance : {:.4f}\".format(acc_manhattan))"
      ],
      "id": "0dc9fef8",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance : 0.9722\n",
            "Accuracy with Manhattan distance : 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2116e22a",
      "metadata": {
        "id": "2116e22a"
      },
      "source": [
        "**Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:  \n",
        "● Use PCA to reduce dimensionality  \n",
        "● Decide how many components to keep  \n",
        "● Use KNN for classification post-dimensionality reduction  \n",
        "● Evaluate the model  \n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data  \n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n",
        "**Answer (Conceptual Explanation):**  \n",
        "\n",
        "1. **Use PCA to reduce dimensionality:**  \n",
        "   - Gene expression datasets often have **thousands of genes (features)** but relatively few patient samples.  \n",
        "   - I would first **standardize** all gene expression features (zero mean, unit variance).  \n",
        "   - Then I would apply **PCA** to transform the data into a lower-dimensional space where each principal component is a linear combination of genes capturing maximum variance.  \n",
        "   - This reduces noise, removes correlations, and alleviates the Curse of Dimensionality.\n",
        "\n",
        "2. **Decide how many components to keep:**  \n",
        "   - Examine the **explained variance ratio** of each principal component.  \n",
        "   - Plot the **cumulative explained variance** and choose the smallest number of components that capture, for example, **90–95%** of the total variance.  \n",
        "   - Alternatively, use cross-validation to see how classification performance changes as we vary the number of components.\n",
        "\n",
        "3. **Use KNN for classification after dimensionality reduction:**  \n",
        "   - After projecting the data into the PCA space, I would train a **KNN classifier** on the principal components instead of the original genes.  \n",
        "   - I would tune **K** (e.g., 3, 5, 7, 9) and possibly the distance metric using cross-validation to find the best configuration.\n",
        "\n",
        "4. **Evaluate the model:**  \n",
        "   - Use **stratified k-fold cross-validation** (e.g., k=5 or 10) due to the small sample size.  \n",
        "   - Compute metrics like **accuracy, precision, recall, F1-score**, and possibly **ROC-AUC** for each cancer type (one-vs-rest).  \n",
        "   - Report **mean and standard deviation** of these metrics across folds to demonstrate stability.\n",
        "\n",
        "5. **Justify this pipeline to stakeholders:**  \n",
        "   - PCA reduces dimensionality from thousands of genes to a much smaller set of components, which:\n",
        "     - Reduces overfitting risk.  \n",
        "     - Makes the model more **robust** and **generalizable**.  \n",
        "     - Improves computational efficiency.  \n",
        "   - KNN in the PCA space leverages **similarity between patients** in a compressed, denoised feature space.  \n",
        "   - Cross-validation-based evaluation provides statistically reliable performance estimates, making the solution suitable for real-world biomedical settings where data is limited and high-dimensional.\n",
        "\n",
        "Below is a **demonstration code** using a synthetic high-dimensional dataset (since the real gene expression data is not provided). It illustrates the PCA + KNN pipeline and how we might evaluate it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3a955e5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a955e5a",
        "outputId": "706d9c78-7079-4bbd-cfdd-5a711b0f3fdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA + KNN pipeline accuracy (5-fold CV):\n",
            "Mean accuracy : 0.5140\n",
            "Std deviation : 0.0350\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
        "from sklearn.decomposition import PCA # Import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier # Import KNeighborsClassifier\n",
        "\n",
        "# Create a synthetic high-dimensional dataset\n",
        "# (e.g., 500 features with 50 informative)\n",
        "X_synth, y_synth = make_classification(\n",
        "    n_samples=500,\n",
        "    n_features=500,\n",
        "    n_informative=50,\n",
        "    n_redundant=50,\n",
        "    n_classes=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Build a pipeline: StandardScaler -> PCA -> KNN\n",
        "pca_knn_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=0.95)),  # keep enough components to explain 95% variance\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "# Use stratified k-fold cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scores = cross_val_score(pca_knn_pipeline, X_synth, y_synth, cv=cv, scoring='accuracy')\n",
        "\n",
        "print(\"PCA + KNN pipeline accuracy (5-fold CV):\")\n",
        "print(\"Mean accuracy : {:.4f}\".format(scores.mean()))\n",
        "print(\"Std deviation : {:.4f}\".format(scores.std()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}